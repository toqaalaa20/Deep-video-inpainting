{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLAuGp7m1SHN"
   },
   "source": [
    "Here is a simplified example of how Deep Video Inpainting (DVF) can be implemented in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_dir= 'C:\\\\dataset2014\\\\results2\\\\baseline\\\\highway'\n",
    "gt_dir= 'C:\\\\dataset2014\\\\dataset\\\\baseline\\\\highway\\\\groundtruth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(data_dir):\n",
    "    lst= []\n",
    "    for frame in os.listdir(data_dir):\n",
    "        frame_path= os.path.join(data_dir, frame)\n",
    "        frame= cv2.imread(frame_path)\n",
    "        frame= frame.reshape(frame.shape[0], frame.shape[1], 1 , frame.shape[2])\n",
    "        lst.append(frame)\n",
    "    return lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames= generate(frames_dir)\n",
    "masks= generate(gt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=np.array(frames)\n",
    "masks= np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700, 240, 320, 1, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "29IEAKKT1SHW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "29IEAKKT1SHW"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class DVF(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DVF, self).__init__()\n",
    "\n",
    "        # Define the encoder network\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv3D(64, (3, 3, 3), strides=(1, 1, 1), padding='same', input_shape=(None, None, None, 3)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv3D(128, (3, 3, 3), strides=(1, 1, 1), padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv3D(256, (3, 3, 3), strides=(1, 1, 1), padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "\n",
    "        # Define the decoder network\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv3DTranspose(128, (3, 3, 3), strides=(1, 1, 1), padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv3DTranspose(64, (3, 3, 3), strides=(1, 1, 1), padding='same'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv3DTranspose(3, (3, 3, 3), strides=(1, 1, 1), padding='same', activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, video, mask):\n",
    "        # Encode the video\n",
    "        encoded_video = self.encoder(video)\n",
    "        print(encoded_video)\n",
    "\n",
    "        # Mask the encoded video\n",
    "#         masked_encoded_video = encoded_video * mask\n",
    "        mask = torch.zeros((video.shape[1], video.shape[2], video.shape[3]))\n",
    "        mask[100:200, 100:200, 100:200] = 1\n",
    "\n",
    "\n",
    "        # Decode the masked encoded video\n",
    "        decoded_video = self.decoder(masked_encoded_video)\n",
    "\n",
    "        # Return the decoded video\n",
    "        return encoded_video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DVF object at 0x00000201926F8AC0>\n"
     ]
    }
   ],
   "source": [
    "fvf= DVF()\n",
    "print(fvf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dvf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks= tf.convert_to_tensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames= tf.convert_to_tensor(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1700, 240, 320, 1, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame= frames[0]\n",
    "mask= masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame= [frame]\n",
    "mask= [mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame= np.array(frame)\n",
    "mask= np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[[1.28794402e-01 1.01628125e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "     0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "   [[3.46473366e-01 0.00000000e+00 2.51853094e-02 ... 0.00000000e+00\n",
      "     0.00000000e+00 2.53683269e-01]]\n",
      "\n",
      "   [[2.52928883e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     7.89900124e-02 2.65549064e-01]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[2.51791334e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     5.68379819e-01 1.56508434e+00]]\n",
      "\n",
      "   [[1.68267035e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     7.25379825e-01 1.50537694e+00]]\n",
      "\n",
      "   [[1.31449533e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     4.79648769e-01 2.18475103e+00]]]\n",
      "\n",
      "\n",
      "  [[[1.89228415e-01 0.00000000e+00 1.29362347e-03 ... 0.00000000e+00\n",
      "     3.30650881e-02 1.38776964e-02]]\n",
      "\n",
      "   [[1.99638665e-01 1.19899651e-02 0.00000000e+00 ... 0.00000000e+00\n",
      "     8.99175704e-02 1.89880028e-01]]\n",
      "\n",
      "   [[2.00430676e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     2.81364977e-01 2.43632078e-01]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[3.28535533e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     1.75095534e+00 1.13676238e+00]]\n",
      "\n",
      "   [[2.79685450e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     1.73539352e+00 1.11767447e+00]]\n",
      "\n",
      "   [[9.55169499e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
      "     1.61635184e+00 1.94087291e+00]]]\n",
      "\n",
      "\n",
      "  [[[0.00000000e+00 0.00000000e+00 2.09602937e-01 ... 0.00000000e+00\n",
      "     1.59542203e-01 0.00000000e+00]]\n",
      "\n",
      "   [[4.03642386e-01 2.46094629e-01 1.97614864e-01 ... 0.00000000e+00\n",
      "     2.75795668e-01 2.55308777e-01]]\n",
      "\n",
      "   [[2.95352548e-01 0.00000000e+00 0.00000000e+00 ... 1.43126268e-02\n",
      "     3.71537626e-01 3.62935483e-01]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[3.18958831e+00 0.00000000e+00 0.00000000e+00 ... 1.19281888e-01\n",
      "     2.57539439e+00 1.62122607e+00]]\n",
      "\n",
      "   [[2.99063182e+00 0.00000000e+00 0.00000000e+00 ... 2.14002833e-01\n",
      "     2.49322915e+00 1.36704135e+00]]\n",
      "\n",
      "   [[8.49624753e-01 0.00000000e+00 0.00000000e+00 ... 4.54853117e-01\n",
      "     2.08839226e+00 2.70696950e+00]]]\n",
      "\n",
      "\n",
      "  ...\n",
      "\n",
      "\n",
      "  [[[7.35371590e-01 0.00000000e+00 1.16103113e+00 ... 0.00000000e+00\n",
      "     1.72934866e+00 0.00000000e+00]]\n",
      "\n",
      "   [[2.89737415e+00 6.52430415e-01 1.65557191e-01 ... 0.00000000e+00\n",
      "     2.02230048e+00 5.59915006e-01]]\n",
      "\n",
      "   [[2.72067642e+00 2.94609606e-01 0.00000000e+00 ... 5.20234108e-01\n",
      "     2.10854459e+00 1.75667310e+00]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[2.78039932e+00 2.98283160e-01 5.25676548e-01 ... 1.21084630e+00\n",
      "     2.34540081e+00 5.21782160e-01]]\n",
      "\n",
      "   [[2.50096321e+00 9.06000733e-01 5.90396404e-01 ... 7.93685079e-01\n",
      "     1.68217766e+00 2.69615412e-01]]\n",
      "\n",
      "   [[1.14089406e+00 0.00000000e+00 0.00000000e+00 ... 7.21053720e-01\n",
      "     1.42313373e+00 1.51640713e+00]]]\n",
      "\n",
      "\n",
      "  [[[2.96928078e-01 0.00000000e+00 1.21978211e+00 ... 0.00000000e+00\n",
      "     1.44151103e+00 7.54462630e-02]]\n",
      "\n",
      "   [[2.57761216e+00 4.33745235e-01 1.57077149e-01 ... 0.00000000e+00\n",
      "     1.66870582e+00 1.08122003e+00]]\n",
      "\n",
      "   [[2.37689567e+00 8.73002052e-01 0.00000000e+00 ... 3.46918762e-01\n",
      "     1.57776308e+00 1.97065699e+00]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[2.31907940e+00 1.47100538e-01 6.70502782e-01 ... 4.39057469e-01\n",
      "     1.44861925e+00 7.24648654e-01]]\n",
      "\n",
      "   [[2.35460091e+00 9.46360588e-01 7.20377386e-01 ... 2.95064449e-01\n",
      "     9.86346722e-01 5.87810695e-01]]\n",
      "\n",
      "   [[9.08054948e-01 3.93965282e-02 0.00000000e+00 ... 1.78960234e-01\n",
      "     9.02720273e-01 1.34603226e+00]]]\n",
      "\n",
      "\n",
      "  [[[6.49906769e-02 0.00000000e+00 1.47570324e+00 ... 0.00000000e+00\n",
      "     1.14212656e+00 4.45930809e-02]]\n",
      "\n",
      "   [[1.00582218e+00 5.40430486e-01 1.09388220e+00 ... 8.29935074e-01\n",
      "     1.76462698e+00 0.00000000e+00]]\n",
      "\n",
      "   [[8.82430375e-01 1.12640381e+00 7.13216066e-01 ... 1.33140659e+00\n",
      "     1.27744818e+00 1.24142386e-01]]\n",
      "\n",
      "   ...\n",
      "\n",
      "   [[6.30847454e-01 6.32074952e-01 9.86792326e-01 ... 1.13325644e+00\n",
      "     1.37673557e+00 0.00000000e+00]]\n",
      "\n",
      "   [[4.61292952e-01 9.05668855e-01 8.13654721e-01 ... 9.31895256e-01\n",
      "     1.02167571e+00 0.00000000e+00]]\n",
      "\n",
      "   [[3.84563096e-02 1.06129222e-01 0.00000000e+00 ... 1.02402735e+00\n",
      "     8.05053294e-01 2.02579632e-01]]]]], shape=(1, 240, 320, 1, 256), dtype=float32)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "required broadcastable shapes [Op:Mul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inpainted_frames \u001b[38;5;241m=\u001b[39m \u001b[43mdvf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\keras\\engine\\base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1040\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mDVF.call\u001b[1;34m(self, video, mask)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_video)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Mask the encoded video\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m masked_encoded_video \u001b[38;5;241m=\u001b[39m \u001b[43mencoded_video\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Decode the masked encoded video\u001b[39;00m\n\u001b[0;32m     40\u001b[0m decoded_video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(masked_encoded_video)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1367\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1362\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1363\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y, force_same_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1367\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1369\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1373\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1710\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1708\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m sparse_tensor\u001b[38;5;241m.\u001b[39mSparseTensor(y\u001b[38;5;241m.\u001b[39mindices, new_vals, y\u001b[38;5;241m.\u001b[39mdense_shape)\n\u001b[0;32m   1709\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:530\u001b[0m, in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.multiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    483\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultiply\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    485\u001b[0m   \u001b[38;5;124;03m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m  For example:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6235\u001b[0m, in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   6233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   6234\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 6235\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   6237\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\deep_learning\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:6941\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6940\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 6941\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: required broadcastable shapes [Op:Mul]"
     ]
    }
   ],
   "source": [
    "inpainted_frames = dvf(frame, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqNaP2Sj1SHe"
   },
   "source": [
    "This is just a basic example, and there are many different ways to implement a DVF model. You can use different encoder and decoder networks, different training algorithms, and different loss functions.\n",
    "\n",
    "You can also add additional features to your DVF model, such as:\n",
    "\n",
    "* The ability to inpaint videos with different types of missing regions, such as holes, occlusions, and noise.\n",
    "* The ability to inpaint videos with different types of backgrounds, such as natural scenes, urban scenes, and indoor scenes.\n",
    "* The ability to fine-tune the DVF model on a specific task, such as inpainting medical videos or inpainting videos for artistic purposes.\n",
    "\n",
    "DVFs are a powerful tool for video restoration, editing, and generation. They are still under development, but they have the potential to revolutionize the way we interact with videos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
